{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data in csv file using pandas\n",
    "df = pd.read_csv('filtered_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813579</td>\n",
       "      <td>Mon Apr 06 22:20:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>starkissed</td>\n",
       "      <td>@LettyA ahh ive always wanted to see rent  lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467818603</td>\n",
       "      <td>Mon Apr 06 22:21:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kennypham</td>\n",
       "      <td>Sad, sad, sad. I don't know why but I hate thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467819650</td>\n",
       "      <td>Mon Apr 06 22:22:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>antzpantz</td>\n",
       "      <td>@Viennah Yay! I'm happy for you with your job!...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag        user  \\\n",
       "0       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY      Karoli   \n",
       "1       0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY        coZZ   \n",
       "2       0  1467813579  Mon Apr 06 22:20:31 PDT 2009  NO_QUERY  starkissed   \n",
       "3       0  1467818603  Mon Apr 06 22:21:49 PDT 2009  NO_QUERY   kennypham   \n",
       "4       0  1467819650  Mon Apr 06 22:22:05 PDT 2009  NO_QUERY   antzpantz   \n",
       "\n",
       "                                                text  \n",
       "0  @nationwideclass no, it's not behaving at all....  \n",
       "1  @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "2  @LettyA ahh ive always wanted to see rent  lov...  \n",
       "3  Sad, sad, sad. I don't know why but I hate thi...  \n",
       "4  @Viennah Yay! I'm happy for you with your job!...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove mentions, emails and links\n",
    "def removeAt(tweet):\n",
    "    words = tweet.split(' ')\n",
    "    return ' '.join(filter(lambda x: x and '@' not in x, words))\n",
    "\n",
    "def removeLinks(tweet):\n",
    "    words = tweet.split(' ')\n",
    "    return ' '.join(filter(lambda x: not ('http://' in x.lower() or 'https://' in x.lower()), words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply formulas to clean data\n",
    "df.text = df.text.apply(lambda x: removeAt(removeLinks(x.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14524</th>\n",
       "      <td>0</td>\n",
       "      <td>1835705341</td>\n",
       "      <td>Mon May 18 06:34:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>12gaBrowningGal</td>\n",
       "      <td>Blind. Definitely. They aren't the smartest bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68041</th>\n",
       "      <td>0</td>\n",
       "      <td>2211076285</td>\n",
       "      <td>Wed Jun 17 12:24:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>12gaBrowningGal</td>\n",
       "      <td>I'm killing me too! I'm hungry now- Pie, Outdo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85046</th>\n",
       "      <td>0</td>\n",
       "      <td>2299986642</td>\n",
       "      <td>Tue Jun 23 13:35:33 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>12gaBrowningGal</td>\n",
       "      <td>My skeet game bites! I am not joking. I will h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203184</th>\n",
       "      <td>4</td>\n",
       "      <td>2071364168</td>\n",
       "      <td>Sun Jun 07 19:16:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>12gaBrowningGal</td>\n",
       "      <td>That's just because she trying to figure out w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207475</th>\n",
       "      <td>4</td>\n",
       "      <td>2178187758</td>\n",
       "      <td>Mon Jun 15 07:19:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>12gaBrowningGal</td>\n",
       "      <td>I am really not skilled enough to shoot trap. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target          id                          date      flag  \\\n",
       "14524        0  1835705341  Mon May 18 06:34:45 PDT 2009  NO_QUERY   \n",
       "68041        0  2211076285  Wed Jun 17 12:24:07 PDT 2009  NO_QUERY   \n",
       "85046        0  2299986642  Tue Jun 23 13:35:33 PDT 2009  NO_QUERY   \n",
       "203184       4  2071364168  Sun Jun 07 19:16:31 PDT 2009  NO_QUERY   \n",
       "207475       4  2178187758  Mon Jun 15 07:19:40 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \n",
       "14524   12gaBrowningGal  Blind. Definitely. They aren't the smartest bi...  \n",
       "68041   12gaBrowningGal  I'm killing me too! I'm hungry now- Pie, Outdo...  \n",
       "85046   12gaBrowningGal  My skeet game bites! I am not joking. I will h...  \n",
       "203184  12gaBrowningGal  That's just because she trying to figure out w...  \n",
       "207475  12gaBrowningGal  I am really not skilled enough to shoot trap. ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaned dataframe \n",
    "df.drop_duplicates(subset='text', keep=False, inplace=True)\n",
    "\n",
    "# sort dataframe by user \n",
    "df.sort_values(by='user', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary consolidating users and tweets to send to LIWC \n",
    "user_list = df['user'].tolist()\n",
    "tweets_list = df['text'].tolist()\n",
    "\n",
    "user_tweet_dic = {}\n",
    "\n",
    "for i in range(len(user_list)):\n",
    "    user = user_list[i]\n",
    "    tweet = tweets_list[i]\n",
    "    \n",
    "    if user not in user_tweet_dic:\n",
    "        user_tweet_dic[user] = []\n",
    "        user_tweet_dic[user].append(tweet)\n",
    "    else:\n",
    "        user_tweet_dic[user].append(tweet)\n",
    "        \n",
    "new_user_list = []\n",
    "new_combined_tweets_list = []\n",
    "\n",
    "for user, tweets in user_tweet_dic.items():\n",
    "    combined_tweets = ' '.join(tweets)\n",
    "    if len(combined_tweets.split(' ')) >= 100:\n",
    "        new_user_list.append(user)\n",
    "        new_combined_tweets_list.append(combined_tweets)\n",
    "    \n",
    "# Create a dataframe based on dictionary and exporting as a csv file\n",
    "consolidated_tweets_df = pd.DataFrame({'user': new_user_list, 'tweets': new_combined_tweets_list})\n",
    "consolidated_tweets_df.to_csv('consolidated_tweets_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    Imagine the group taking that csv file and putting it through the LIWC software to get the output! :D\n",
    "    <br>\n",
    "    The output will be saved in a file called LIWC2015 Results (consolidated_tweets_df.csv).csv that will be imported later\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for the first 500 users \n",
    "first_500_users = consolidated_tweets_df['user'].unique()[:500]\n",
    "first_500_users_df = df[df['user'].isin(first_500_users)]\n",
    "\n",
    "# Creating a dictionary of user and their tweets \n",
    "user_tweet_dictionary = {}\n",
    "\n",
    "# Create a format that follows IBM Watson's format and saving it to the user_tweet_dictionary\n",
    "for index, row in first_500_users_df.iterrows():\n",
    "    user = row['user']\n",
    "    text = row['text']\n",
    "    \n",
    "    tweet = {}\n",
    "    \n",
    "    tweet['content'] = text\n",
    "    tweet['contenttype'] = 'text/plain'\n",
    "    tweet['language'] = 'en'\n",
    "    \n",
    "    if user not in user_tweet_dictionary:\n",
    "        user_tweet_dictionary[user] = {'contentItems': []}\n",
    "        user_tweet_dictionary[user]['contentItems'].append(tweet)\n",
    "    else:\n",
    "        user_tweet_dictionary[user]['contentItems'].append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    The bottom few cells are commented out because we are using API calls to get the output, and after a certain number of calls, we will have to pay money!\n",
    "    <br>\n",
    "    Instead, we have saved the file called big5_df.csv that will also be imported later\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from ibm_watson import PersonalityInsightsV3\n",
    "# from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "# from credentials import IBM_KEY\n",
    "\n",
    "# #API KEY provided on the service page \n",
    "# KEY = IBM_KEY\n",
    "\n",
    "# # Authentication via IBM's IAM (Identity and Access Management)\n",
    "# authenticator = IAMAuthenticator(KEY)\n",
    "\n",
    "# # Creating a service instance\n",
    "# service = PersonalityInsightsV3(\n",
    "#     version='2017-10-13',\n",
    "#     authenticator=authenticator)\n",
    "\n",
    "# # Setting service endpoint \n",
    "# service.set_service_url('https://gateway.watsonplatform.net/personality-insights/api')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a dictionary to store results from IBM Watson\n",
    "# results = {'User': [],\n",
    "#           'Openness': [],\n",
    "#           'Conscientiousness': [],\n",
    "#           'Extraversion': [],\n",
    "#           'Agreeableness': [],\n",
    "#           'Emotional range': []}\n",
    "\n",
    "# # creates profiles of users in user_tweet_dictionary and saves them to results\n",
    "# for user, tweets in user_tweet_dictionary.items():\n",
    "#     profile = service.profile(tweets, 'application/json', raw_scores=True, consumption_preferences=True).get_result()\n",
    "    \n",
    "#     results['User'].append(user)\n",
    "#     results['Openness'].append(profile['personality'][0]['raw_score'])\n",
    "#     results['Conscientiousness'].append(profile['personality'][1]['raw_score'])\n",
    "#     results['Extraversion'].append(profile['personality'][2]['raw_score'])\n",
    "#     results['Agreeableness'].append(profile['personality'][3]['raw_score'])\n",
    "#     results['Emotional range'].append(profile['personality'][4]['raw_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dataframe from results dictionary and save csv to use\n",
    "# big5_df = pd.DataFrame(results)\n",
    "# big5_df.to_csv('big5_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "      <th>Extraversion</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Emotional range</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12gaBrowningGal</td>\n",
       "      <td>0.764999</td>\n",
       "      <td>0.656144</td>\n",
       "      <td>0.548956</td>\n",
       "      <td>0.788804</td>\n",
       "      <td>0.439579</td>\n",
       "      <td>388</td>\n",
       "      <td>27.36</td>\n",
       "      <td>34.94</td>\n",
       "      <td>68.96</td>\n",
       "      <td>...</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>12.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15Stepz</td>\n",
       "      <td>0.748124</td>\n",
       "      <td>0.690716</td>\n",
       "      <td>0.564859</td>\n",
       "      <td>0.823930</td>\n",
       "      <td>0.410770</td>\n",
       "      <td>254</td>\n",
       "      <td>26.07</td>\n",
       "      <td>70.94</td>\n",
       "      <td>84.80</td>\n",
       "      <td>...</td>\n",
       "      <td>9.45</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.33</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16_MileyCyrus</td>\n",
       "      <td>0.672814</td>\n",
       "      <td>0.637773</td>\n",
       "      <td>0.525612</td>\n",
       "      <td>0.811025</td>\n",
       "      <td>0.563704</td>\n",
       "      <td>1164</td>\n",
       "      <td>21.04</td>\n",
       "      <td>64.42</td>\n",
       "      <td>58.80</td>\n",
       "      <td>...</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.15</td>\n",
       "      <td>13.57</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18percentgrey</td>\n",
       "      <td>0.789412</td>\n",
       "      <td>0.607883</td>\n",
       "      <td>0.528168</td>\n",
       "      <td>0.772518</td>\n",
       "      <td>0.461528</td>\n",
       "      <td>931</td>\n",
       "      <td>47.25</td>\n",
       "      <td>53.87</td>\n",
       "      <td>58.83</td>\n",
       "      <td>...</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.22</td>\n",
       "      <td>1.29</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19fischi75</td>\n",
       "      <td>0.706904</td>\n",
       "      <td>0.627109</td>\n",
       "      <td>0.447471</td>\n",
       "      <td>0.803232</td>\n",
       "      <td>0.518279</td>\n",
       "      <td>1707</td>\n",
       "      <td>48.43</td>\n",
       "      <td>48.83</td>\n",
       "      <td>63.95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.64</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.58</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              User  Openness  Conscientiousness  Extraversion  Agreeableness  \\\n",
       "0  12gaBrowningGal  0.764999           0.656144      0.548956       0.788804   \n",
       "1          15Stepz  0.748124           0.690716      0.564859       0.823930   \n",
       "2    16_MileyCyrus  0.672814           0.637773      0.525612       0.811025   \n",
       "3    18percentgrey  0.789412           0.607883      0.528168       0.772518   \n",
       "4       19fischi75  0.706904           0.627109      0.447471       0.803232   \n",
       "\n",
       "   Emotional range    WC  Analytic  Clout  Authentic  ...  Comma  Colon  \\\n",
       "0         0.439579   388     27.36  34.94      68.96  ...   2.58   0.00   \n",
       "1         0.410770   254     26.07  70.94      84.80  ...   9.45   0.39   \n",
       "2         0.563704  1164     21.04  64.42      58.80  ...   3.26   0.52   \n",
       "3         0.461528   931     47.25  53.87      58.83  ...   4.83   0.64   \n",
       "4         0.518279  1707     48.43  48.83      63.95  ...   0.70   0.23   \n",
       "\n",
       "   SemiC  QMark  Exclam  Dash  Quote  Apostro  Parenth  OtherP  \n",
       "0   0.00   0.52   12.37  0.52    0.0     3.61     0.00    0.00  \n",
       "1   0.00   1.18    0.39  1.57    0.0     4.33     1.57    0.00  \n",
       "2   0.00   2.15   13.57  0.43    0.0     1.37     0.26    1.03  \n",
       "3   0.75   1.61    2.26  1.93    0.0     3.22     1.29    2.26  \n",
       "4   0.00   3.10    1.64  3.87    0.0     0.00     2.58    3.81  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing both the outputs of LIWC and IBM Watson (Big 5)\n",
    "LIWC_df = pd.read_csv('LIWC2015 Results (consolidated_tweets_df.csv).csv')\n",
    "LIWC_df.drop(0, inplace=True)\n",
    "LIWC_df.drop(columns=['A', 'C'], inplace=True)\n",
    "LIWC_df.rename(columns={'B': 'User'}, inplace=True)\n",
    "big5_df = pd.read_csv('big5_df.csv').drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Merge dataframes together\n",
    "updated_df = pd.merge(big5_df, LIWC_df)\n",
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(number):\n",
    "    return round(number*100,2)\n",
    "\n",
    "updated_df['Openness'] = updated_df['Openness'].apply(normalise)\n",
    "updated_df['Conscientiousness'] = updated_df['Conscientiousness'].apply(normalise)\n",
    "updated_df['Extraversion'] = updated_df['Extraversion'].apply(normalise)\n",
    "updated_df['Agreeableness'] = updated_df['Agreeableness'].apply(normalise)\n",
    "updated_df['Emotional range'] = updated_df['Emotional range'].apply(normalise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_score(row):\n",
    "    sixletter = row['Sixltr']\n",
    "    wps = row['WPS'] \n",
    "    wordcount = row['WC']\n",
    "    \n",
    "    readability = (-1 * sixletter) + wps - wordcount + 3\n",
    "    return readability \n",
    "\n",
    "updated_df['readability'] = updated_df.apply(readability_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_percentile(score):\n",
    "    readability_range = updated_df['readability'].max() - updated_df['readability'].min()\n",
    "    return -score/readability_range*100\n",
    "\n",
    "updated_df['readability'] = updated_df['readability'].apply(readability_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_score(row):    \n",
    "    #intermediate numbers\n",
    "    sixletter = row['Sixltr']\n",
    "    articles = row['article']\n",
    "    pasttense = row['focuspast']\n",
    "    presenttense = row['focuspresent']\n",
    "    firstperson = 100 - row['i'] #inverse of first person singular pronoun\n",
    "    discrepancy = row['discrep']\n",
    "    wps = row['WPS'] #words per sentence\n",
    "    wordcount = row['WC']\n",
    "    \n",
    "    #features\n",
    "    #Distancing (0.19) - six letter words, articles, past tense, and the inverse of first person singular pronouns, present tense and discrepancy words\n",
    "    \n",
    "    Distancing = (sixletter + articles + pasttense + firstperson + presenttense + discrepancy)/6 #average of 6 intermediate numbers\n",
    "    \n",
    "    #Readability (-0.32) - multiplying negative one by six letter words, adding words per sentence, subtracting the amount of words recognized by the LIWC dictionary, and adding three\n",
    "#     Readability = (-1 * sixletter) + wps - wordcount + 3\n",
    "    Readability = row['readability']\n",
    "    \n",
    "    #Swear (0.31) \n",
    "    Swear = row['swear']\n",
    "    \n",
    "    #Anger (0.22)\n",
    "    Anger = row['anger']\n",
    "    \n",
    "    #Agreeability (-0.43)\n",
    "    Agreeability = row['Agreeableness']\n",
    "        \n",
    "    #Neuroticism (0.3)\n",
    "    Neuroticism = row['Emotional range']\n",
    "    \n",
    "    #calculate composite score\n",
    "    Total = 0.19 + 0.32 + 0.31 + 0.22 + 0.3 + 0.43\n",
    "    Psychopathy_Score = 0.19/Total * Distancing - 0.32/Total * Readability + 0.31/Total * Swear + 0.22/Total * Anger - 0.43/Total * Agreeability + 0.3/Total * Neuroticism\n",
    "    \n",
    "    #write into dictionary\n",
    "    return Psychopathy_Score\n",
    "\n",
    "updated_df['composite score'] = updated_df.apply(composite_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_percentile(score):\n",
    "    max_score = updated_df['composite score'].max()\n",
    "    min_score = updated_df['composite score'].min()\n",
    "    score_range = max_score - min_score\n",
    "    return (score-min_score)/score_range*100\n",
    "\n",
    "updated_df['composite score percentile'] = updated_df['composite score'].apply(score_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = updated_df[['User', 'Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Emotional range', 'WC', 'Analytic', 'Clout', 'Authentic', 'Tone', 'WPS', 'Sixltr', 'Dic', 'function', 'pronoun', 'ppron', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'article', 'prep', 'auxverb', 'adverb', 'conj', 'negate', 'verb', 'adj', 'compare', 'interrog', 'number', 'quant', 'affect', 'posemo', 'negemo', 'anx', 'anger', 'sad', 'social', 'family', 'friend', 'female', 'male', 'cogproc', 'insight', 'cause', 'discrep', 'tentat', 'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'bio', 'body', 'health', 'sexual', 'ingest', 'drives', 'affiliation', 'achieve', 'power', 'reward', 'risk', 'focuspast', 'focuspresent', 'focusfuture', 'relativ', 'motion', 'space', 'time', 'work', 'leisure', 'home', 'money', 'relig', 'death', 'informal', 'swear', 'netspeak', 'assent', 'nonflu', 'filler', 'AllPunc', 'Period', 'Comma', 'Colon', 'SemiC', 'QMark', 'Exclam', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP', 'readability', 'composite score', 'composite score percentile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "      <th>Extraversion</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Emotional range</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>...</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>readability</th>\n",
       "      <th>composite score</th>\n",
       "      <th>composite score percentile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abby_ox</td>\n",
       "      <td>64.52</td>\n",
       "      <td>51.86</td>\n",
       "      <td>39.56</td>\n",
       "      <td>66.06</td>\n",
       "      <td>77.37</td>\n",
       "      <td>305</td>\n",
       "      <td>56.42</td>\n",
       "      <td>4.43</td>\n",
       "      <td>97.80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.30</td>\n",
       "      <td>13.664762</td>\n",
       "      <td>-2.891426</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BethanyAnn614</td>\n",
       "      <td>67.97</td>\n",
       "      <td>56.48</td>\n",
       "      <td>49.19</td>\n",
       "      <td>72.73</td>\n",
       "      <td>87.82</td>\n",
       "      <td>379</td>\n",
       "      <td>54.79</td>\n",
       "      <td>23.01</td>\n",
       "      <td>73.69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.06</td>\n",
       "      <td>17.784407</td>\n",
       "      <td>-2.944168</td>\n",
       "      <td>99.754655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Benjimonicus</td>\n",
       "      <td>68.73</td>\n",
       "      <td>52.83</td>\n",
       "      <td>42.49</td>\n",
       "      <td>63.95</td>\n",
       "      <td>76.37</td>\n",
       "      <td>385</td>\n",
       "      <td>59.27</td>\n",
       "      <td>26.65</td>\n",
       "      <td>78.40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>17.613205</td>\n",
       "      <td>-3.044657</td>\n",
       "      <td>99.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiGVixXen</td>\n",
       "      <td>68.36</td>\n",
       "      <td>50.65</td>\n",
       "      <td>42.85</td>\n",
       "      <td>72.13</td>\n",
       "      <td>81.86</td>\n",
       "      <td>225</td>\n",
       "      <td>9.74</td>\n",
       "      <td>8.43</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.44</td>\n",
       "      <td>11.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.407696</td>\n",
       "      <td>-3.155676</td>\n",
       "      <td>98.770759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DaniScot</td>\n",
       "      <td>69.67</td>\n",
       "      <td>50.53</td>\n",
       "      <td>41.53</td>\n",
       "      <td>67.40</td>\n",
       "      <td>75.14</td>\n",
       "      <td>289</td>\n",
       "      <td>65.89</td>\n",
       "      <td>9.44</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>13.353784</td>\n",
       "      <td>-3.633829</td>\n",
       "      <td>96.546479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CoConutShelle</td>\n",
       "      <td>63.47</td>\n",
       "      <td>57.43</td>\n",
       "      <td>47.24</td>\n",
       "      <td>72.90</td>\n",
       "      <td>81.17</td>\n",
       "      <td>289</td>\n",
       "      <td>60.29</td>\n",
       "      <td>18.41</td>\n",
       "      <td>53.02</td>\n",
       "      <td>...</td>\n",
       "      <td>2.08</td>\n",
       "      <td>7.61</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.433522</td>\n",
       "      <td>-3.821409</td>\n",
       "      <td>95.673893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BonjourHoney</td>\n",
       "      <td>75.07</td>\n",
       "      <td>50.75</td>\n",
       "      <td>43.40</td>\n",
       "      <td>74.96</td>\n",
       "      <td>72.85</td>\n",
       "      <td>171</td>\n",
       "      <td>6.03</td>\n",
       "      <td>59.24</td>\n",
       "      <td>50.35</td>\n",
       "      <td>...</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.616863</td>\n",
       "      <td>-4.538181</td>\n",
       "      <td>92.339604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bastante_P</td>\n",
       "      <td>69.27</td>\n",
       "      <td>49.89</td>\n",
       "      <td>46.49</td>\n",
       "      <td>68.52</td>\n",
       "      <td>68.92</td>\n",
       "      <td>305</td>\n",
       "      <td>43.39</td>\n",
       "      <td>14.70</td>\n",
       "      <td>80.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.66</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.66</td>\n",
       "      <td>13.180705</td>\n",
       "      <td>-4.573028</td>\n",
       "      <td>92.177502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ACsBarbieGirl69</td>\n",
       "      <td>66.75</td>\n",
       "      <td>52.23</td>\n",
       "      <td>44.40</td>\n",
       "      <td>70.72</td>\n",
       "      <td>70.38</td>\n",
       "      <td>306</td>\n",
       "      <td>33.80</td>\n",
       "      <td>13.32</td>\n",
       "      <td>62.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.98</td>\n",
       "      <td>14.71</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>14.223398</td>\n",
       "      <td>-4.757017</td>\n",
       "      <td>91.321616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Beccixo</td>\n",
       "      <td>68.74</td>\n",
       "      <td>55.63</td>\n",
       "      <td>47.50</td>\n",
       "      <td>67.37</td>\n",
       "      <td>65.93</td>\n",
       "      <td>269</td>\n",
       "      <td>83.28</td>\n",
       "      <td>27.61</td>\n",
       "      <td>81.57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.74</td>\n",
       "      <td>10.827494</td>\n",
       "      <td>-4.775903</td>\n",
       "      <td>91.233765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AllTheSausages</td>\n",
       "      <td>79.36</td>\n",
       "      <td>59.97</td>\n",
       "      <td>52.89</td>\n",
       "      <td>71.15</td>\n",
       "      <td>72.50</td>\n",
       "      <td>268</td>\n",
       "      <td>41.72</td>\n",
       "      <td>30.06</td>\n",
       "      <td>1.94</td>\n",
       "      <td>...</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.99</td>\n",
       "      <td>34.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.12</td>\n",
       "      <td>13.116915</td>\n",
       "      <td>-4.924970</td>\n",
       "      <td>90.540334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Beever</td>\n",
       "      <td>71.94</td>\n",
       "      <td>60.73</td>\n",
       "      <td>50.76</td>\n",
       "      <td>72.95</td>\n",
       "      <td>71.10</td>\n",
       "      <td>243</td>\n",
       "      <td>81.03</td>\n",
       "      <td>50.00</td>\n",
       "      <td>47.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.65</td>\n",
       "      <td>10.266044</td>\n",
       "      <td>-4.939417</td>\n",
       "      <td>90.473129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BrookeAmanda</td>\n",
       "      <td>68.39</td>\n",
       "      <td>54.93</td>\n",
       "      <td>44.07</td>\n",
       "      <td>67.30</td>\n",
       "      <td>66.60</td>\n",
       "      <td>334</td>\n",
       "      <td>51.69</td>\n",
       "      <td>19.27</td>\n",
       "      <td>86.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.99</td>\n",
       "      <td>15.497800</td>\n",
       "      <td>-5.141363</td>\n",
       "      <td>89.533711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BisForBecca</td>\n",
       "      <td>65.87</td>\n",
       "      <td>53.65</td>\n",
       "      <td>47.01</td>\n",
       "      <td>72.37</td>\n",
       "      <td>68.81</td>\n",
       "      <td>215</td>\n",
       "      <td>63.06</td>\n",
       "      <td>64.52</td>\n",
       "      <td>40.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>8.385163</td>\n",
       "      <td>-5.171997</td>\n",
       "      <td>89.391206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BreakDancingAmy</td>\n",
       "      <td>71.55</td>\n",
       "      <td>53.68</td>\n",
       "      <td>49.16</td>\n",
       "      <td>69.91</td>\n",
       "      <td>73.53</td>\n",
       "      <td>409</td>\n",
       "      <td>42.84</td>\n",
       "      <td>54.86</td>\n",
       "      <td>45.32</td>\n",
       "      <td>...</td>\n",
       "      <td>2.69</td>\n",
       "      <td>12.96</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.93</td>\n",
       "      <td>19.020816</td>\n",
       "      <td>-5.225251</td>\n",
       "      <td>89.143480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Alisha_Herself</td>\n",
       "      <td>68.73</td>\n",
       "      <td>59.71</td>\n",
       "      <td>42.81</td>\n",
       "      <td>68.83</td>\n",
       "      <td>68.22</td>\n",
       "      <td>343</td>\n",
       "      <td>36.04</td>\n",
       "      <td>11.05</td>\n",
       "      <td>83.41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>14.936350</td>\n",
       "      <td>-5.302881</td>\n",
       "      <td>88.782362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ClaireOJD</td>\n",
       "      <td>65.65</td>\n",
       "      <td>62.50</td>\n",
       "      <td>53.03</td>\n",
       "      <td>70.67</td>\n",
       "      <td>66.17</td>\n",
       "      <td>209</td>\n",
       "      <td>51.07</td>\n",
       "      <td>23.65</td>\n",
       "      <td>97.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.310594</td>\n",
       "      <td>-5.436680</td>\n",
       "      <td>88.159953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DWsLala</td>\n",
       "      <td>66.01</td>\n",
       "      <td>56.16</td>\n",
       "      <td>46.64</td>\n",
       "      <td>70.97</td>\n",
       "      <td>70.92</td>\n",
       "      <td>336</td>\n",
       "      <td>20.59</td>\n",
       "      <td>32.77</td>\n",
       "      <td>97.44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.19</td>\n",
       "      <td>14.175555</td>\n",
       "      <td>-5.495402</td>\n",
       "      <td>87.886790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Caiteee</td>\n",
       "      <td>69.32</td>\n",
       "      <td>55.26</td>\n",
       "      <td>45.52</td>\n",
       "      <td>70.14</td>\n",
       "      <td>69.85</td>\n",
       "      <td>352</td>\n",
       "      <td>48.98</td>\n",
       "      <td>16.70</td>\n",
       "      <td>81.69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.57</td>\n",
       "      <td>12.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.311129</td>\n",
       "      <td>-5.561861</td>\n",
       "      <td>87.577632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CaptainClairesy</td>\n",
       "      <td>67.03</td>\n",
       "      <td>53.95</td>\n",
       "      <td>46.43</td>\n",
       "      <td>73.44</td>\n",
       "      <td>71.70</td>\n",
       "      <td>281</td>\n",
       "      <td>53.93</td>\n",
       "      <td>28.45</td>\n",
       "      <td>94.51</td>\n",
       "      <td>...</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>12.991679</td>\n",
       "      <td>-5.612808</td>\n",
       "      <td>87.340636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AlmaLaCubana</td>\n",
       "      <td>67.56</td>\n",
       "      <td>53.88</td>\n",
       "      <td>44.12</td>\n",
       "      <td>75.78</td>\n",
       "      <td>75.41</td>\n",
       "      <td>322</td>\n",
       "      <td>24.89</td>\n",
       "      <td>23.75</td>\n",
       "      <td>68.63</td>\n",
       "      <td>...</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.24</td>\n",
       "      <td>15.070498</td>\n",
       "      <td>-5.759393</td>\n",
       "      <td>86.658751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Boy_Kill_Boy</td>\n",
       "      <td>72.59</td>\n",
       "      <td>52.48</td>\n",
       "      <td>43.54</td>\n",
       "      <td>71.24</td>\n",
       "      <td>62.11</td>\n",
       "      <td>232</td>\n",
       "      <td>15.19</td>\n",
       "      <td>28.76</td>\n",
       "      <td>62.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>10.137994</td>\n",
       "      <td>-5.760409</td>\n",
       "      <td>86.654023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Charlotte__C</td>\n",
       "      <td>63.97</td>\n",
       "      <td>53.63</td>\n",
       "      <td>45.74</td>\n",
       "      <td>74.78</td>\n",
       "      <td>71.20</td>\n",
       "      <td>247</td>\n",
       "      <td>20.69</td>\n",
       "      <td>28.54</td>\n",
       "      <td>91.27</td>\n",
       "      <td>...</td>\n",
       "      <td>4.05</td>\n",
       "      <td>7.29</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.403953</td>\n",
       "      <td>-5.786204</td>\n",
       "      <td>86.534030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AndrewBoland</td>\n",
       "      <td>65.73</td>\n",
       "      <td>61.56</td>\n",
       "      <td>50.45</td>\n",
       "      <td>77.22</td>\n",
       "      <td>73.36</td>\n",
       "      <td>218</td>\n",
       "      <td>66.11</td>\n",
       "      <td>57.29</td>\n",
       "      <td>70.97</td>\n",
       "      <td>...</td>\n",
       "      <td>1.83</td>\n",
       "      <td>6.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.131896</td>\n",
       "      <td>-5.884495</td>\n",
       "      <td>86.076799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AliciaVelasquez</td>\n",
       "      <td>64.23</td>\n",
       "      <td>62.60</td>\n",
       "      <td>49.13</td>\n",
       "      <td>77.41</td>\n",
       "      <td>74.39</td>\n",
       "      <td>231</td>\n",
       "      <td>34.82</td>\n",
       "      <td>12.13</td>\n",
       "      <td>83.48</td>\n",
       "      <td>...</td>\n",
       "      <td>1.73</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>10.814829</td>\n",
       "      <td>-5.893331</td>\n",
       "      <td>86.035698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4_s_m_4</td>\n",
       "      <td>64.91</td>\n",
       "      <td>59.52</td>\n",
       "      <td>44.13</td>\n",
       "      <td>70.77</td>\n",
       "      <td>64.44</td>\n",
       "      <td>225</td>\n",
       "      <td>10.30</td>\n",
       "      <td>41.22</td>\n",
       "      <td>43.37</td>\n",
       "      <td>...</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2.22</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.212103</td>\n",
       "      <td>-5.962169</td>\n",
       "      <td>85.715474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DalekGirl93</td>\n",
       "      <td>71.09</td>\n",
       "      <td>54.66</td>\n",
       "      <td>47.82</td>\n",
       "      <td>74.35</td>\n",
       "      <td>71.57</td>\n",
       "      <td>286</td>\n",
       "      <td>27.38</td>\n",
       "      <td>35.01</td>\n",
       "      <td>42.91</td>\n",
       "      <td>...</td>\n",
       "      <td>1.40</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.70</td>\n",
       "      <td>4.20</td>\n",
       "      <td>13.382865</td>\n",
       "      <td>-5.993870</td>\n",
       "      <td>85.568008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>BrandySanDiego</td>\n",
       "      <td>68.87</td>\n",
       "      <td>55.49</td>\n",
       "      <td>50.65</td>\n",
       "      <td>73.00</td>\n",
       "      <td>73.04</td>\n",
       "      <td>443</td>\n",
       "      <td>34.81</td>\n",
       "      <td>17.73</td>\n",
       "      <td>81.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.837888</td>\n",
       "      <td>-6.037537</td>\n",
       "      <td>85.364877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Brenna_Bee</td>\n",
       "      <td>68.27</td>\n",
       "      <td>57.84</td>\n",
       "      <td>53.61</td>\n",
       "      <td>70.23</td>\n",
       "      <td>58.20</td>\n",
       "      <td>241</td>\n",
       "      <td>80.07</td>\n",
       "      <td>22.76</td>\n",
       "      <td>96.57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.077008</td>\n",
       "      <td>-6.073894</td>\n",
       "      <td>85.195751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Cladia</td>\n",
       "      <td>69.12</td>\n",
       "      <td>53.71</td>\n",
       "      <td>49.68</td>\n",
       "      <td>71.37</td>\n",
       "      <td>69.67</td>\n",
       "      <td>360</td>\n",
       "      <td>25.89</td>\n",
       "      <td>36.95</td>\n",
       "      <td>91.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.94</td>\n",
       "      <td>16.564883</td>\n",
       "      <td>-6.145261</td>\n",
       "      <td>84.863764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               User  Openness  Conscientiousness  Extraversion  Agreeableness  \\\n",
       "0           Abby_ox     64.52              51.86         39.56          66.06   \n",
       "1     BethanyAnn614     67.97              56.48         49.19          72.73   \n",
       "2      Benjimonicus     68.73              52.83         42.49          63.95   \n",
       "3         BiGVixXen     68.36              50.65         42.85          72.13   \n",
       "4          DaniScot     69.67              50.53         41.53          67.40   \n",
       "5     CoConutShelle     63.47              57.43         47.24          72.90   \n",
       "6      BonjourHoney     75.07              50.75         43.40          74.96   \n",
       "7        Bastante_P     69.27              49.89         46.49          68.52   \n",
       "8   ACsBarbieGirl69     66.75              52.23         44.40          70.72   \n",
       "9           Beccixo     68.74              55.63         47.50          67.37   \n",
       "10   AllTheSausages     79.36              59.97         52.89          71.15   \n",
       "11           Beever     71.94              60.73         50.76          72.95   \n",
       "12     BrookeAmanda     68.39              54.93         44.07          67.30   \n",
       "13      BisForBecca     65.87              53.65         47.01          72.37   \n",
       "14  BreakDancingAmy     71.55              53.68         49.16          69.91   \n",
       "15   Alisha_Herself     68.73              59.71         42.81          68.83   \n",
       "16        ClaireOJD     65.65              62.50         53.03          70.67   \n",
       "17          DWsLala     66.01              56.16         46.64          70.97   \n",
       "18          Caiteee     69.32              55.26         45.52          70.14   \n",
       "19  CaptainClairesy     67.03              53.95         46.43          73.44   \n",
       "20     AlmaLaCubana     67.56              53.88         44.12          75.78   \n",
       "21     Boy_Kill_Boy     72.59              52.48         43.54          71.24   \n",
       "22     Charlotte__C     63.97              53.63         45.74          74.78   \n",
       "23     AndrewBoland     65.73              61.56         50.45          77.22   \n",
       "24  AliciaVelasquez     64.23              62.60         49.13          77.41   \n",
       "25          4_s_m_4     64.91              59.52         44.13          70.77   \n",
       "26      DalekGirl93     71.09              54.66         47.82          74.35   \n",
       "27   BrandySanDiego     68.87              55.49         50.65          73.00   \n",
       "28       Brenna_Bee     68.27              57.84         53.61          70.23   \n",
       "29           Cladia     69.12              53.71         49.68          71.37   \n",
       "\n",
       "    Emotional range   WC  Analytic  Clout  Authentic  ...  QMark  Exclam  \\\n",
       "0             77.37  305     56.42   4.43      97.80  ...   0.00    0.66   \n",
       "1             87.82  379     54.79  23.01      73.69  ...   0.26    3.43   \n",
       "2             76.37  385     59.27  26.65      78.40  ...   1.04    1.04   \n",
       "3             81.86  225      9.74   8.43      99.00  ...   0.44   11.56   \n",
       "4             75.14  289     65.89   9.44      99.00  ...   0.35    5.88   \n",
       "5             81.17  289     60.29  18.41      53.02  ...   2.08    7.61   \n",
       "6             72.85  171      6.03  59.24      50.35  ...   1.75    0.00   \n",
       "7             68.92  305     43.39  14.70      80.82  ...   0.66    3.93   \n",
       "8             70.38  306     33.80  13.32      62.28  ...   0.98   14.71   \n",
       "9             65.93  269     83.28  27.61      81.57  ...   0.00    0.00   \n",
       "10            72.50  268     41.72  30.06       1.94  ...   1.49    2.99   \n",
       "11            71.10  243     81.03  50.00      47.17  ...   0.00    2.47   \n",
       "12            66.60  334     51.69  19.27      86.24  ...   0.90    4.49   \n",
       "13            68.81  215     63.06  64.52      40.35  ...   0.47    0.00   \n",
       "14            73.53  409     42.84  54.86      45.32  ...   2.69   12.96   \n",
       "15            68.22  343     36.04  11.05      83.41  ...   0.00    1.75   \n",
       "16            66.17  209     51.07  23.65      97.70  ...   0.48    2.87   \n",
       "17            70.92  336     20.59  32.77      97.44  ...   0.89    0.89   \n",
       "18            69.85  352     48.98  16.70      81.69  ...   0.57   12.50   \n",
       "19            71.70  281     53.93  28.45      94.51  ...   0.71    3.20   \n",
       "20            75.41  322     24.89  23.75      68.63  ...   2.80    9.01   \n",
       "21            62.11  232     15.19  28.76      62.09  ...   0.43    2.16   \n",
       "22            71.20  247     20.69  28.54      91.27  ...   4.05    7.29   \n",
       "23            73.36  218     66.11  57.29      70.97  ...   1.83    6.42   \n",
       "24            74.39  231     34.82  12.13      83.48  ...   1.73    5.63   \n",
       "25            64.44  225     10.30  41.22      43.37  ...   8.00    2.22   \n",
       "26            71.57  286     27.38  35.01      42.91  ...   1.40    9.44   \n",
       "27            73.04  443     34.81  17.73      81.64  ...   0.68    1.81   \n",
       "28            58.20  241     80.07  22.76      96.57  ...   0.00    2.90   \n",
       "29            69.67  360     25.89  36.95      91.31  ...   0.56    0.56   \n",
       "\n",
       "     Dash  Quote  Apostro  Parenth  OtherP  readability  composite score  \\\n",
       "0    0.00    0.0     3.28     0.00    2.30    13.664762        -2.891426   \n",
       "1    0.00    0.0     1.06     0.53    1.06    17.784407        -2.944168   \n",
       "2    0.26    0.0     4.68     0.00    0.52    17.613205        -3.044657   \n",
       "3    0.00    0.0     8.00     0.00    0.00    10.407696        -3.155676   \n",
       "4    0.00    0.0     3.11     0.69    0.69    13.353784        -3.633829   \n",
       "5    0.35    0.0     2.08     0.00    0.00    13.433522        -3.821409   \n",
       "6    0.00    0.0     5.85     0.00    0.00     7.616863        -4.538181   \n",
       "7    0.00    0.0     4.92     0.33    0.66    13.180705        -4.573028   \n",
       "8    0.33    0.0     0.00     0.00    0.98    14.223398        -4.757017   \n",
       "9    0.37    0.0     0.37     0.37    0.74    10.827494        -4.775903   \n",
       "10  34.33    0.0     0.75     0.00    1.12    13.116915        -4.924970   \n",
       "11   0.41    0.0     1.65     0.00    1.65    10.266044        -4.939417   \n",
       "12   0.00    0.0     2.69     0.60    2.99    15.497800        -5.141363   \n",
       "13   0.00    0.0     0.47     0.00    0.47     8.385163        -5.171997   \n",
       "14   2.93    0.0     2.20     0.24    2.93    19.020816        -5.225251   \n",
       "15   0.58    0.0     0.29     0.00    0.58    14.936350        -5.302881   \n",
       "16   1.44    0.0     4.31     0.00    0.00     9.310594        -5.436680   \n",
       "17   0.30    0.0     4.46     0.00    1.19    14.175555        -5.495402   \n",
       "18   0.00    0.0     0.85     0.00    0.00    16.311129        -5.561861   \n",
       "19   0.71    0.0     4.27     0.00    0.71    12.991679        -5.612808   \n",
       "20   0.00    0.0     5.28     0.00    1.24    15.070498        -5.759393   \n",
       "21   0.00    0.0     3.45     0.00    0.43    10.137994        -5.760409   \n",
       "22   0.40    0.0     0.00     0.00    0.00    11.403953        -5.786204   \n",
       "23   0.00    0.0     1.83     0.00    0.00    10.131896        -5.884495   \n",
       "24   0.87    0.0     3.90     0.00    0.43    10.814829        -5.893331   \n",
       "25   1.33    0.0     1.33     0.00    0.00    10.212103        -5.962169   \n",
       "26   0.00    0.0     4.90     0.70    4.20    13.382865        -5.993870   \n",
       "27   0.23    0.0     2.26     0.00    0.00    18.837888        -6.037537   \n",
       "28   0.41    0.0     0.41     0.00    0.00     9.077008        -6.073894   \n",
       "29   0.00    0.0     7.78     0.00    1.94    16.564883        -6.145261   \n",
       "\n",
       "    composite score percentile  \n",
       "0                   100.000000  \n",
       "1                    99.754655  \n",
       "2                    99.287200  \n",
       "3                    98.770759  \n",
       "4                    96.546479  \n",
       "5                    95.673893  \n",
       "6                    92.339604  \n",
       "7                    92.177502  \n",
       "8                    91.321616  \n",
       "9                    91.233765  \n",
       "10                   90.540334  \n",
       "11                   90.473129  \n",
       "12                   89.533711  \n",
       "13                   89.391206  \n",
       "14                   89.143480  \n",
       "15                   88.782362  \n",
       "16                   88.159953  \n",
       "17                   87.886790  \n",
       "18                   87.577632  \n",
       "19                   87.340636  \n",
       "20                   86.658751  \n",
       "21                   86.654023  \n",
       "22                   86.534030  \n",
       "23                   86.076799  \n",
       "24                   86.035698  \n",
       "25                   85.715474  \n",
       "26                   85.568008  \n",
       "27                   85.364877  \n",
       "28                   85.195751  \n",
       "29                   84.863764  \n",
       "\n",
       "[30 rows x 102 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df = updated_df.sort_values(by='composite score', ascending=False).reset_index(drop=True)\n",
    "updated_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\steve\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# boy_kill_boy and up are psychopaths\n",
    "updated_df['psychopath'] = list(range(500))\n",
    "updated_df['psychopath'][0:22] = 1\n",
    "updated_df['psychopath'][22:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace features by text of user \n",
    "updated_df = updated_df[['User','psychopath']]\n",
    "updated_df = pd.merge(updated_df,consolidated_tweets_df.rename(columns={'user':'User'}),how='left')[['User','tweets','psychopath']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>tweets</th>\n",
       "      <th>psychopath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abby_ox</td>\n",
       "      <td>wooo!! finished my lamp for Dt #LoveEverybody ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BethanyAnn614</td>\n",
       "      <td>ugh. I don't like today.i much prefer tomorrow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Benjimonicus</td>\n",
       "      <td>Walking to stow This family guy is OLD Morning...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiGVixXen</td>\n",
       "      <td>no I didn't have time! I rushed outta the hous...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DaniScot</td>\n",
       "      <td>depends on the kind of video.... I'm pretty su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            User                                             tweets  \\\n",
       "0        Abby_ox  wooo!! finished my lamp for Dt #LoveEverybody ...   \n",
       "1  BethanyAnn614  ugh. I don't like today.i much prefer tomorrow...   \n",
       "2   Benjimonicus  Walking to stow This family guy is OLD Morning...   \n",
       "3      BiGVixXen  no I didn't have time! I rushed outta the hous...   \n",
       "4       DaniScot  depends on the kind of video.... I'm pretty su...   \n",
       "\n",
       "   psychopath  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "from sklearn import model_selection, naive_bayes\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "word_Lemmatized = WordNetLemmatizer()\n",
    "\n",
    "def preprocessing(text):\n",
    "    final_words = []\n",
    "    # lowercase\n",
    "    text = p.clean(text)\n",
    "    text = text.lower()\n",
    "    # tokenize\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    for word, tag in pos_tag(text):\n",
    "        # remove stopwords and only keep alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            # lemmatize words\n",
    "            word_final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            # append final words to final_words list\n",
    "            final_words.append(word_final)\n",
    "            \n",
    "    # return final string\n",
    "    return ' '.join(final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df['cleaned_text'] = updated_df['tweets'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>tweets</th>\n",
       "      <th>psychopath</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abby_ox</td>\n",
       "      <td>wooo!! finished my lamp for Dt #LoveEverybody ...</td>\n",
       "      <td>1</td>\n",
       "      <td>wooo finish lamp dt ca believe today really ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BethanyAnn614</td>\n",
       "      <td>ugh. I don't like today.i much prefer tomorrow...</td>\n",
       "      <td>1</td>\n",
       "      <td>ugh like much prefer tomorrow boo leave work h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Benjimonicus</td>\n",
       "      <td>Walking to stow This family guy is OLD Morning...</td>\n",
       "      <td>1</td>\n",
       "      <td>walk stow family guy old morning get sleep lli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiGVixXen</td>\n",
       "      <td>no I didn't have time! I rushed outta the hous...</td>\n",
       "      <td>1</td>\n",
       "      <td>time rush outta house hair ruin rain already p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DaniScot</td>\n",
       "      <td>depends on the kind of video.... I'm pretty su...</td>\n",
       "      <td>1</td>\n",
       "      <td>depends kind video pretty sure whole tonight c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CoConutShelle</td>\n",
       "      <td>I want more followers... Follow me please! x i...</td>\n",
       "      <td>1</td>\n",
       "      <td>want follower follow please x rele want lipsy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BonjourHoney</td>\n",
       "      <td>No, I just heard about it from someone Who bat...</td>\n",
       "      <td>1</td>\n",
       "      <td>hear someone bath puppy toilet jaw hurt like h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bastante_P</td>\n",
       "      <td>u forgot to add I'm also confined to a very sm...</td>\n",
       "      <td>1</td>\n",
       "      <td>u forget add also confine small part small as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ACsBarbieGirl69</td>\n",
       "      <td>ugh that sucks hun it does indeed!!!! See ya h...</td>\n",
       "      <td>1</td>\n",
       "      <td>ugh suck hun indeed see ya hun ill call u poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Beccixo</td>\n",
       "      <td>i really hate revising it is so boring and dos...</td>\n",
       "      <td>1</td>\n",
       "      <td>really hate revise boring dosnt go ur head bea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              User                                             tweets  \\\n",
       "0          Abby_ox  wooo!! finished my lamp for Dt #LoveEverybody ...   \n",
       "1    BethanyAnn614  ugh. I don't like today.i much prefer tomorrow...   \n",
       "2     Benjimonicus  Walking to stow This family guy is OLD Morning...   \n",
       "3        BiGVixXen  no I didn't have time! I rushed outta the hous...   \n",
       "4         DaniScot  depends on the kind of video.... I'm pretty su...   \n",
       "5    CoConutShelle  I want more followers... Follow me please! x i...   \n",
       "6     BonjourHoney  No, I just heard about it from someone Who bat...   \n",
       "7       Bastante_P  u forgot to add I'm also confined to a very sm...   \n",
       "8  ACsBarbieGirl69  ugh that sucks hun it does indeed!!!! See ya h...   \n",
       "9          Beccixo  i really hate revising it is so boring and dos...   \n",
       "\n",
       "   psychopath                                       cleaned_text  \n",
       "0           1  wooo finish lamp dt ca believe today really ni...  \n",
       "1           1  ugh like much prefer tomorrow boo leave work h...  \n",
       "2           1  walk stow family guy old morning get sleep lli...  \n",
       "3           1  time rush outta house hair ruin rain already p...  \n",
       "4           1  depends kind video pretty sure whole tonight c...  \n",
       "5           1  want follower follow please x rele want lipsy ...  \n",
       "6           1  hear someone bath puppy toilet jaw hurt like h...  \n",
       "7           1  u forget add also confine small part small as ...  \n",
       "8           1  ugh suck hun indeed see ya hun ill call u poin...  \n",
       "9           1  really hate revise boring dosnt go ur head bea...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    updated_df['cleaned_text'],updated_df['psychopath'],test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer()\n",
    "Tfidf_vect.fit(updated_df['cleaned_text'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(updated_df['cleaned_text'])\n",
    "X_train = cv.transform(X_train)\n",
    "X_test = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9466666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# logistic regression & cv\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "# logistic regression & Tf-idf\n",
    "lr = LogisticRegression()\n",
    "lr.fit(Train_X_Tfidf, y_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, lr.predict(Test_X_Tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naive bayes\n",
    "nb = naive_bayes.MultinomialNB()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression & cv\n",
      "\n",
      "Words that predict psychopathy\n",
      "\n",
      "('sausage', 0.6277481174226662)\n",
      "('tai', 0.4634724226341537)\n",
      "('ruin', 0.3667804121958295)\n",
      "('bye', 0.2891842181374162)\n",
      "('ugh', 0.28741289347150106)\n",
      "('fuk', 0.28369162306603196)\n",
      "('fukin', 0.28369162306603196)\n",
      "('argh', 0.2826925270853313)\n",
      "('bacon', 0.2781631249721203)\n",
      "('brain', 0.27778190910892475)\n",
      "======================================\n",
      "Words that predict non-psychopathy\n",
      "\n",
      "('good', -0.501469942780011)\n",
      "('lol', -0.4145327869797684)\n",
      "('work', -0.39758410450642157)\n",
      "('quot', -0.39049547576999843)\n",
      "('love', -0.38471334190137174)\n",
      "('thanks', -0.3234263535041009)\n",
      "('well', -0.3128893019731597)\n",
      "('would', -0.2884208311964785)\n",
      "('amp', -0.2875480584555549)\n",
      "('one', -0.2786353644233663)\n"
     ]
    }
   ],
   "source": [
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), lr.coef_[0]\n",
    "    )\n",
    "}\n",
    "\n",
    "print('Logistic Regression & cv\\n')\n",
    "print('Words that predict psychopathy\\n')\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:10]:\n",
    "    print (best_positive)\n",
    "    \n",
    "print('======================================')\n",
    "print('Words that predict non-psychopathy\\n')\n",
    "    \n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1])[:10]:\n",
    "    print (best_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression & tf-idf\n",
      "\n",
      "Words that predict psychopathy\n",
      "\n",
      "('sausage', 0.6277481174226662)\n",
      "('tai', 0.4634724226341537)\n",
      "('ruin', 0.3667804121958295)\n",
      "('bye', 0.2891842181374162)\n",
      "('ugh', 0.28741289347150106)\n",
      "('fuk', 0.28369162306603196)\n",
      "('fukin', 0.28369162306603196)\n",
      "('argh', 0.2826925270853313)\n",
      "('bacon', 0.2781631249721203)\n",
      "('brain', 0.27778190910892475)\n",
      "======================================\n",
      "Words that predict non-psychopathy\n",
      "\n",
      "('good', -0.501469942780011)\n",
      "('lol', -0.4145327869797684)\n",
      "('work', -0.39758410450642157)\n",
      "('quot', -0.39049547576999843)\n",
      "('love', -0.38471334190137174)\n",
      "('thanks', -0.3234263535041009)\n",
      "('well', -0.3128893019731597)\n",
      "('would', -0.2884208311964785)\n",
      "('amp', -0.2875480584555549)\n",
      "('one', -0.2786353644233663)\n"
     ]
    }
   ],
   "source": [
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        Tfidf_vect.get_feature_names(), lr.coef_[0]\n",
    "    )\n",
    "}\n",
    "\n",
    "print('Logistic Regression & tf-idf\\n')\n",
    "print('Words that predict psychopathy\\n')\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:10]:\n",
    "    print (best_positive)\n",
    "    \n",
    "print('======================================')\n",
    "print('Words that predict non-psychopathy\\n')\n",
    "    \n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1])[:10]:\n",
    "    print (best_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "\n",
      "Words that predict psychopathy\n",
      "\n",
      "('go', -6.911215950611117)\n",
      "('get', -7.134359501925328)\n",
      "('like', -7.221370878914957)\n",
      "('day', -7.316681058719282)\n",
      "('feel', -7.316681058719282)\n",
      "('lol', -7.422041574377108)\n",
      "('miss', -7.422041574377108)\n",
      "('try', -7.422041574377108)\n",
      "('wait', -7.422041574377108)\n",
      "('ca', -7.539824610033492)\n",
      "======================================\n",
      "Words that predict non-psychopathy\n",
      "\n",
      "('aa', -9.619266151713328)\n",
      "('aaa', -9.619266151713328)\n",
      "('aaaaaaaaaaaaaaaahhhhhhh', -9.619266151713328)\n",
      "('aaaaaaaaaaaalbum', -9.619266151713328)\n",
      "('aaaaaaaaah', -9.619266151713328)\n",
      "('aaaaaaahhh', -9.619266151713328)\n",
      "('aaaaaaaw', -9.619266151713328)\n",
      "('aaaaaah', -9.619266151713328)\n",
      "('aaaaah', -9.619266151713328)\n",
      "('aaaah', -9.619266151713328)\n"
     ]
    }
   ],
   "source": [
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), nb.coef_[0]\n",
    "    )\n",
    "}\n",
    "\n",
    "print('Naive Bayes\\n')\n",
    "print('Words that predict psychopathy\\n')\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:10]:\n",
    "    print (best_positive)\n",
    "    \n",
    "print('======================================')\n",
    "print('Words that predict non-psychopathy\\n')\n",
    "    \n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1])[:10]:\n",
    "    print (best_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08056563689431563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You are normal'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_psychopath(input_string):\n",
    "    \n",
    "    lower_string = input_string.lower()\n",
    "    token_string = word_tokenize(lower_string)\n",
    "    \n",
    "    final_words = []\n",
    "    for word, tag in pos_tag(token_string):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            final_words.append(word_final)\n",
    "            \n",
    "    transformed_string = Tfidf_vect.transform(final_words)\n",
    "    \n",
    "    if lr.predict(transformed_string)[0] == 1:\n",
    "        print(lr.predict_proba(transformed_string)[0][1])\n",
    "        return \"Psychopathic!!!\"\n",
    "    else:\n",
    "        print(lr.predict_proba(transformed_string)[0][1])\n",
    "        return \"You are normal\"\n",
    "\n",
    "predict_psychopath(\"gah. can't sleep. super anxious about getting Lolas shots done in the morning I hate watching my babies go through it! ugh its the worst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files\n",
    "import pickle\n",
    "pickle.dump(lr, open('logistic_regression_psychopath_model.sav', 'wb'))\n",
    "pickle.dump(cv, open('text_transformation_model.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
